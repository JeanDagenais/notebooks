{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "from PIL import Image\n",
    "import unittest\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "class Data():\n",
    "    @staticmethod\n",
    "    def lidar_array():\n",
    "        arr = np.random.randint(0,2, size=365)\n",
    "        return arr\n",
    "    \n",
    "    @staticmethod\n",
    "    def camera_img():\n",
    "        img = Image.open('/home/wroscoe/mydonkey_old2/sessions/1115am/frame_00001_ttl_0.42665756334488203_agl_0.09659284488019566_mil_0.0.jpg')\n",
    "        return img\n",
    "    \n",
    "    def camera_arr(self):\n",
    "        img = self.camera_img()\n",
    "        return np.array(img)\n",
    "    \n",
    "    @staticmethod\n",
    "    def steering_angle():\n",
    "        return np.random.random() * 2 - 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def throttle():\n",
    "        return np.random.random() * 1.2 - .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Tub():\n",
    "    \"\"\"\n",
    "    A datastore to store sensor data in a key, value format.\n",
    "    \n",
    "    Accepts str, int, float, image_array, image, and array data types.\n",
    "    \n",
    "    For example:\n",
    "    \n",
    "    #Create a tub to store speed values.\n",
    "    >>> path = '~/mydonkey/test_tub'\n",
    "    >>> inputs = ['user/speed', 'cam/image']\n",
    "    >>> types = ['float', 'image']\n",
    "    >>> t=Tub(path=path, inputs=inputs, types=types)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, path, meta=None):\n",
    "               \n",
    "        self.path = os.path.expanduser(path)\n",
    "        self.meta_path = os.path.join(self.path, 'meta.json')\n",
    "        \n",
    "        exists = os.path.exists(self.path)\n",
    "        \n",
    "        if exists:\n",
    "            #load log and meta\n",
    "            with open(self.meta_path, 'r') as f:\n",
    "                self.meta = json.load(f)\n",
    "                \n",
    "            self.current_ix = self.get_last_ix() + 1\n",
    "                \n",
    "        elif not exists and meta:\n",
    "            #create log and save meta\n",
    "            os.makedirs(self.path)    \n",
    "            self.meta = meta\n",
    "            with open(self.meta_path, 'w') as f:\n",
    "                json.dump(self.meta, f)\n",
    "            self.current_ix = 0\n",
    "        else:\n",
    "            raise AttributeError('The path doesnt exist and you pass meta info.')\n",
    "        \n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        \n",
    "    def get_last_ix(self):\n",
    "        index = self.get_index()\n",
    "        return max(index)\n",
    "    \n",
    "    def get_index(self):\n",
    "        files = next(os.walk(self.path))[2]\n",
    "        record_files = [f for f in files if f[:6]=='record']\n",
    "        print(record_files)\n",
    "        def get_file_ix(file_name):\n",
    "            try:\n",
    "                name = file_name.split('.')[0]\n",
    "                num = int(name.split('_')[1])\n",
    "            except:\n",
    "                num = 0\n",
    "            return num\n",
    "        \n",
    "        nums = [get_file_ix(f) for f in record_files]\n",
    "        return nums\n",
    "    \n",
    "    @property\n",
    "    def inputs(self):\n",
    "        return list(self.meta.keys())\n",
    "    \n",
    "    @property\n",
    "    def types(self):\n",
    "        return list(self.meta.values())\n",
    "\n",
    "        \n",
    "    def get_input_type(self, key):\n",
    "        return self.meta.get(key)\n",
    "    \n",
    "    def write_json_record(self, json_data):\n",
    "        path = self.get_json_record_path(self.current_ix)\n",
    "        with open(path, 'w') as fp:\n",
    "            json.dump(json_data, fp)\n",
    "            \n",
    "        print(path)\n",
    "            \n",
    "    def get_json_record_path(self, ix):\n",
    "        return os.path.join(self.path, 'record_'+str(ix)+'.json')\n",
    "    \n",
    "    def get_json_record(self, ix):\n",
    "        path = self.get_json_record_path(ix)\n",
    "        with open(path, 'r') as fp:\n",
    "            json_data = json.load(fp)\n",
    "        return json_data\n",
    "        \n",
    "    def put_record(self, data):\n",
    "        \"\"\"\n",
    "        Save values like images that can't be saved in the csv log and\n",
    "        return a record with references to the saved values that can\n",
    "        be saved in a csv.\n",
    "        \"\"\"\n",
    "        \n",
    "        json_data = {}\n",
    "        \n",
    "        line = []\n",
    "        \n",
    "        for key, val in data.items():\n",
    "            typ = self.get_input_type(key)\n",
    "            \n",
    "            if typ in ['str', 'float', 'int']:\n",
    "                json_data[key] = val       \n",
    "\n",
    "            elif typ is 'image':\n",
    "                path = self.make_file_path(key)\n",
    "                val.save(path)\n",
    "                json_data[key]=path\n",
    "                \n",
    "            elif typ == 'image_array':\n",
    "                path = self.make_file_path(key, ext='.png')\n",
    "                img = Image.fromarray(np.uint8(val))\n",
    "                img.save(path)\n",
    "                json_data[key]=path\n",
    "\n",
    "            else:\n",
    "                msg = 'Tub does not know what to do with this type {}'.format(typ)\n",
    "                raise TypeError(msg)\n",
    "        \n",
    "        #write csv line\n",
    "        self.write_json_record(json_data)\n",
    "        self.current_ix += 1\n",
    "    \n",
    "    def get_record(self, ix):\n",
    "        \n",
    "        json_data = self.get_json_record(ix)\n",
    "        data={}\n",
    "        for key, val in json_data.items():\n",
    "            typ = self.get_input_type(key)\n",
    "            \n",
    "            #load objects that were saved as separate files\n",
    "            if typ == 'image':\n",
    "                val = Image.open(val)\n",
    "            elif typ == 'image_array':\n",
    "                img = Image.open(val)\n",
    "                val = np.array(img)\n",
    "            \n",
    "            data[key] = val\n",
    "            \n",
    "        return data\n",
    "            \n",
    "    @staticmethod\n",
    "    def clean_file_name(name):\n",
    "        name = name.replace('/', '-')\n",
    "        return name\n",
    "    \n",
    "    def make_file_path(self, key, ext='.png'):\n",
    "        name = '_'.join([str(self.current_ix), key, ext])\n",
    "        name = self.clean_file_name(name)\n",
    "        file_path = os.path.join(self.path, name)\n",
    "        return file_path\n",
    "        \n",
    "    def delete(self):\n",
    "        \"\"\" Delete the folder and files for this tub. \"\"\"\n",
    "        import shutil\n",
    "        shutil.rmtree(self.path)\n",
    "        \n",
    "    def record_gen(self, index):\n",
    "        while True:\n",
    "            for i in index:\n",
    "                record = self.get_record(i)\n",
    "                yield record\n",
    "                \n",
    "    def batch_gen(self, keys, index, batch_size=32):\n",
    "        record_gen = self.record_gen(index)\n",
    "        while True:\n",
    "            record_list = []\n",
    "            for _ in range(batch_size):\n",
    "                record_list.append(next(record_gen))\n",
    "            \n",
    "            batch_arrays = {} \n",
    "            for i, k in enumerate(keys):\n",
    "                arr = np.array([r[k] for r in record_list])\n",
    "                #if len(arr.shape) == 1:\n",
    "                #    arr = arr.reshape(arr.shape + (1,))\n",
    "                batch_arrays[k] = arr\n",
    "                \n",
    "            yield batch_arrays\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['record_1.json', 'record_6.json', 'record_4.json', 'record_3.json', 'record_5.json', 'record_2.json']\n"
     ]
    }
   ],
   "source": [
    "t = Tub('/home/wroscoe/donk2/data/tub_test2', \n",
    "        meta={'x':'float', 'y':'float', 'z':'image_array'})\n",
    "\n",
    "d = Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['record_1.json', 'record_6.json', 'record_4.json', 'record_3.json', 'record_5.json', 'record_2.json']\n"
     ]
    }
   ],
   "source": [
    "index = t.get_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_gen = t.record_gen(index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gen = t.batch_gen(keys=['x', 'z', 'y'], index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = next(batch_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense, merge\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, SimpleRNN, Reshape, BatchNormalization\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.regularizers import l2\n",
    "\n",
    "img_in = Input(shape=(120, 160,3), name='img_in')\n",
    "x = img_in\n",
    "x = Convolution2D(24, (5,5), strides=(2,2), activation='relu')(x)\n",
    "x = Convolution2D(32, (5,5), strides=(2,2), activation='relu')(x)\n",
    "x = Convolution2D(64, (5,5), strides=(2,2), activation='relu')(x)\n",
    "x = Convolution2D(64, (3,3), strides=(2,2), activation='relu')(x)\n",
    "x = Convolution2D(64, (3,3), strides=(1,1), activation='relu')(x)\n",
    "\n",
    "x = Flatten(name='flattened')(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "x = Dropout(.1)(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dropout(.1)(x)\n",
    "#categorical output of the angle\n",
    "angle_out = Dense(1, activation='relu', name='angle_out')(x)\n",
    "\n",
    "#continous output of throttle\n",
    "throttle_out = Dense(1, activation='relu', name='throttle_out')(x)\n",
    "\n",
    "model = Model(inputs=[img_in], outputs=[angle_out, throttle_out])\n",
    "\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss={'angle_out': 'mean_absolute_error', \n",
    "                    'throttle_out': 'mean_absolute_error'},\n",
    "              loss_weights={'angle_out': 0.9, 'throttle_out': .1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 0s - loss: 541131.3750 - angle_out_loss: 600639.8750 - throttle_out_loss: 5555.0000\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s - loss: 606924.9375 - angle_out_loss: 673743.8125 - throttle_out_loss: 5555.0000\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s - loss: 592784.6875 - angle_out_loss: 657673.7500 - throttle_out_loss: 8783.3340\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s - loss: 710084.2500 - angle_out_loss: 788365.3125 - throttle_out_loss: 5555.0000\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s - loss: 497573.4062 - angle_out_loss: 550685.0000 - throttle_out_loss: 19569.0312\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s - loss: 568609.3750 - angle_out_loss: 631171.0000 - throttle_out_loss: 5555.0000\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s - loss: 681123.5625 - angle_out_loss: 756186.7500 - throttle_out_loss: 5555.0000\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s - loss: 624823.1875 - angle_out_loss: 693630.7500 - throttle_out_loss: 5555.0000\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s - loss: 664742.7500 - angle_out_loss: 737985.8750 - throttle_out_loss: 5555.0000\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s - loss: 682091.0625 - angle_out_loss: 757261.7500 - throttle_out_loss: 5555.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcae57f9128>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = '/home/wroscoe/donk2/models/test_model'\n",
    "\n",
    "#checkpoint to save model after each epoch\n",
    "save_best = keras.callbacks.ModelCheckpoint(model_path, monitor='loss', verbose=1, \n",
    "                                      save_best_only=True, mode='min')\n",
    "\n",
    "#stop training if the validation error stops improving.\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='loss', min_delta=.0005, patience=4, \n",
    "                                     verbose=1, mode='auto')\n",
    "\n",
    "callbacks_list = [save_best, early_stop]\n",
    "steps =10\n",
    "epochs=10\n",
    "model.fit(x = arr['z'], y=[arr['x'], arr['y']], epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_gen(gen, X_keys, y_keys):\n",
    "    while True:\n",
    "        batch = next(gen)\n",
    "        X = [batch[k] for k in X_keys]\n",
    "        y = [batch[k] for k in y_keys]\n",
    "        yield X, y\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tg = train_gen(batch_gen, ['z'], ['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wroscoe/envs/donkey/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., steps_per_epoch=10, callbacks=[<keras.ca..., verbose=1, epochs=10, validation_steps=2.0)`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 588298.0556 - angle_out_loss: 653004.8368 - throttle_out_loss: 5937.1248Epoch 00000: loss improved from inf to 604441.82500, saving model to /home/wroscoe/donk2/models/test_model\n",
      "10/10 [==============================] - 4s - loss: 604441.8250 - angle_out_loss: 670940.6031 - throttle_out_loss: 5952.9059     \n",
      "Epoch 2/10\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 600664.8164 - angle_out_loss: 666767.9062 - throttle_out_loss: 5737.1855"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-142161276f69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                         nb_val_samples=steps*.2)\n\u001b[0m",
      "\u001b[0;32m~/envs/donkey/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/donkey/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1888\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1889\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/donkey/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1631\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1633\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1634\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/donkey/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2229\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/donkey/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/donkey/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/donkey/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/envs/donkey/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/donkey/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(\n",
    "                        tg, \n",
    "                        steps_per_epoch=steps, \n",
    "                        nb_epoch=epochs, \n",
    "                        verbose=1, \n",
    "                        callbacks=callbacks_list, \n",
    "                        nb_val_samples=steps*.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image.open()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
